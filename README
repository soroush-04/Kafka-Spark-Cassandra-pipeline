* Setup and Prepare:
====================

- make sure to have latest version of Docker installed

- inside main folder, run "docker compose up -d"

- wait for a couple of mins, then run "docker ps", you should find 4 docker containers : "kafka", "zookeeper", "spark_master", and "cassandra"

- run "docker exec -it spark_master bash", this will get you into "spark_master" container, to run the following:
-- apt-get install libgl1 libglib2.0-0 -y
-- cd /home
-- git clone https://github.com/ultralytics/yolov5.git
-- cd yolov5
-- pip install -r requirements.txt
-- pip3 install kafka-python
-- exit

- run "docker exec -it kafka bash", this will get you into "kafka" container, to run the following:
-- kafka-topics.sh --create --topic pothole --partitions 1 --replication-factor 1 -bootstrap-server localhost:9092
-- exit

- run "docker exec -it cassandra bash", this will get you into "cassandra" container, to run the following:
-- cqlsh -u cassandra -p cassandra
-- CREATE KEYSPACE ph WITH replication = {'class':'SimpleStrategy', 'replication_factor' : 1};
-- create table ph.results(
        id ascii primary key,
        name text,
        content text, 
        cord_thres text);


========================

* Running:
===========
- open new terminal window then run "docker exec -it spark_master bash", this will get you into "spark_master" container, to run the following:
-- cd /home/producer/
-- python3 producer.py

keep this termainl open as it will monitor new images into "producer/images" folder to push them over to Kafka

- open new second terminal window then copy one or images to "app/producer/images" folder inside the main folder
this will automatically trigger pushing it into kafka in the previous termainl window, you should shall some printing indicating that

- open new third terminal window then run "docker exec -it spark_master bash", this will get you into "spark_master" container, to run the following:
-- cd /home/
-- spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,com.datastax.spark:spark-cassandra-connector_2.12:3.0.0 streamingKafka2Console.py

this will submit job to Spark to monitor Kafka for any new messages. When new image appears on Kafka, it will be automatically picked/streamed and processed to check for pothole

the output of check will be saved into "app/runs/detect/exp/", and result will be saved to database "cassandra" too

you should keep this termainl window open to keep Spark monitoring Kafka

- if you would like to check the results into database, in new terminal window run "docker exec -it cassandra bash" which will get you into "cassandra" container, to run the following:
-- cqlsh -u cassandra -p cassandra
-- select * from ph.results;

